{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SketchRNN Model Benchmarking System\n",
    "## Comprehensive Model Evaluation with Hardware-Aware Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add benchmark system to path\n",
    "sys.path.append('./benchmark_system')\n",
    "\n",
    "# Import benchmark components\n",
    "from benchmark_system.core.benchmark_engine import BenchmarkEngine, BenchmarkConfig\n",
    "from benchmark_system.core.model_registry import get_global_registry, register_model\n",
    "from benchmark_system.utils.hardware_detector import create_hardware_detector\n",
    "from benchmark_system.utils.visualization import create_visualizer\n",
    "\n",
    "print(\"‚úÖ Benchmark system imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware Detection\n",
    "hardware_detector = create_hardware_detector()\n",
    "hardware_summary = hardware_detector.get_device_summary()\n",
    "\n",
    "print(\"üñ•Ô∏è HARDWARE DETECTION:\")\n",
    "print(f\"Environment: {hardware_summary['environment']}\")\n",
    "print(f\"Primary Device: {hardware_summary['primary_device']['name']}\")\n",
    "print(f\"Device Type: {hardware_summary['primary_device']['type']}\")\n",
    "print(f\"Memory: {hardware_summary['primary_device']['memory_gb']} GB\")\n",
    "\n",
    "opt_config = hardware_detector.get_optimization_config()\n",
    "print(f\"\\n‚öôÔ∏è OPTIMIZATION: Batch Size: {opt_config.max_batch_size}, Workers: {opt_config.max_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "def load_test_data():\n",
    "    # Load categories\n",
    "    with open('data/categories.json', 'r') as f:\n",
    "        categories = json.load(f)\n",
    "    \n",
    "    # Try to load existing data or create sample data\n",
    "    data_path = 'data/quickdraw_data.pkl'\n",
    "    if Path(data_path).exists():\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "    else:\n",
    "        # Create sample data\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        data = {}\n",
    "        for category in categories:\n",
    "            data[category] = np.random.randint(0, 255, (100, 28, 28), dtype=np.uint8)\n",
    "    \n",
    "    # Prepare test set\n",
    "    test_data, test_labels = [], []\n",
    "    for i, category in enumerate(categories):\n",
    "        if category in data:\n",
    "            samples = data[category][-20:]  # Last 20 for testing\n",
    "            test_data.extend(samples)\n",
    "            test_labels.extend([i] * len(samples))\n",
    "    \n",
    "    return np.array(test_data), np.array(test_labels), categories\n",
    "\n",
    "test_data, test_labels, categories = load_test_data()\n",
    "print(f\"üìä Test data: {test_data.shape}, Labels: {test_labels.shape}, Categories: {len(categories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Registration\n",
    "def create_demo_model():\n",
    "    \"\"\"Create a demonstration CNN model\"\"\"\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from tensorflow.keras import layers, models\n",
    "        \n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(len(categories), activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    except ImportError:\n",
    "        return None\n",
    "\n",
    "# Register models\n",
    "registry = get_global_registry()\n",
    "model_ids = []\n",
    "\n",
    "# Look for existing models\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for model_file in models_dir.glob('*.h5'):\n",
    "    model_id = f\"sketchrnn_{model_file.stem}\"\n",
    "    if register_model(model_id=model_id, name=model_file.stem, model_path=str(model_file), categories=categories):\n",
    "        model_ids.append(model_id)\n",
    "        print(f\"‚úÖ Registered: {model_id}\")\n",
    "\n",
    "# Create demo model if none found\n",
    "if not model_ids:\n",
    "    demo_model = create_demo_model()\n",
    "    if demo_model:\n",
    "        demo_path = models_dir / 'demo_sketchrnn.h5'\n",
    "        demo_model.save(demo_path)\n",
    "        if register_model(model_id=\"demo_sketchrnn\", name=\"Demo SketchRNN\", model_path=str(demo_path), categories=categories):\n",
    "            model_ids.append(\"demo_sketchrnn\")\n",
    "            print(\"‚úÖ Created and registered demo model\")\n",
    "\n",
    "print(f\"üìã Total registered models: {len(model_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmark\n",
    "if model_ids:\n",
    "    config = BenchmarkConfig(\n",
    "        batch_size=32,\n",
    "        max_samples=200,  # Limited for demo\n",
    "        warmup_runs=2,\n",
    "        parallel_models=True,\n",
    "        output_dir=\"benchmark_results_notebook\"\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Starting benchmark...\")\n",
    "    benchmark_engine = BenchmarkEngine(hardware_detector=hardware_detector)\n",
    "    \n",
    "    try:\n",
    "        summary = benchmark_engine.benchmark_models(model_ids, test_data, test_labels, config)\n",
    "        \n",
    "        print(f\"\\n‚úÖ BENCHMARK COMPLETED!\")\n",
    "        print(f\"Total: {summary.total_models}, Successful: {summary.successful_models}, Failed: {summary.failed_models}\")\n",
    "        print(f\"Total Time: {summary.total_execution_time:.2f}s\")\n",
    "        \n",
    "        # Show results\n",
    "        successful_results = [r for r in summary.results if r.error is None and r.metrics]\n",
    "        \n",
    "        if successful_results:\n",
    "            print(\"\\nüìä RESULTS:\")\n",
    "            for result in successful_results:\n",
    "                print(f\"‚Ä¢ {result.model_name}: Accuracy={result.metrics.accuracy:.4f}, Time={result.metrics.inference_time_mean:.4f}s\")\n",
    "        \n",
    "        benchmark_summary = summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Benchmark failed: {e}\")\n",
    "        benchmark_summary = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No models to benchmark\")\n",
    "    benchmark_summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Results\n",
    "if benchmark_summary and benchmark_summary.successful_models > 0:\n",
    "    successful_results = [r for r in benchmark_summary.results if r.error is None and r.metrics]\n",
    "    \n",
    "    # Extract data\n",
    "    model_names = [r.model_name for r in successful_results]\n",
    "    accuracies = [r.metrics.accuracy for r in successful_results]\n",
    "    inference_times = [r.metrics.inference_time_mean for r in successful_results]\n",
    "    \n",
    "    # Create plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Accuracy chart\n",
    "    ax1.bar(model_names, accuracies, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Model Accuracy Comparison')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speed chart\n",
    "    ax2.bar(model_names, inference_times, color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Inference Time Comparison')\n",
    "    ax2.set_ylabel('Time (seconds)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate report\n",
    "    print(\"üìÑ Generating comprehensive report...\")\n",
    "    visualizer = create_visualizer(config.output_dir)\n",
    "    report_dir = visualizer.generate_comprehensive_report(benchmark_summary)\n",
    "    print(f\"‚úÖ Report generated: {report_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "### Command Line Usage:\n",
    "```bash\n",
    "# Discover and benchmark all models\n",
    "python benchmark_models.py benchmark --discover-models --generate-reports\n",
    "\n",
    "# Benchmark specific models\n",
    "python benchmark_models.py benchmark --model-ids model1 model2 --parallel\n",
    "\n",
    "# Register a new model\n",
    "python benchmark_models.py register my_model_id /path/to/model.h5\n",
    "\n",
    "# List all registered models\n",
    "python benchmark_models.py list\n",
    "```\n",
    "\n",
    "### Python API Usage:\n",
    "```python\n",
    "from benchmark_system.core.benchmark_engine import quick_benchmark\n",
    "\n",
    "# Quick benchmark\n",
    "summary = quick_benchmark(\n",
    "    model_ids=['model1', 'model2'],\n",
    "    test_data=test_data,\n",
    "    test_labels=test_labels,\n",
    "    batch_size=64,\n",
    "    parallel=True\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mime_type": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}